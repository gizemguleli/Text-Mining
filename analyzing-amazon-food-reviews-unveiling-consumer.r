{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/guleli8759/analyzing-amazon-food-reviews-unveiling-consumer?scriptVersionId=157153033\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"a1dc48f2","metadata":{"execution":{"iopub.execute_input":"2023-12-30T23:50:34.768724Z","iopub.status.busy":"2023-12-30T23:50:34.767315Z","iopub.status.idle":"2023-12-30T23:50:34.782231Z"},"papermill":{"duration":0.002602,"end_time":"2023-12-30T23:51:08.457762","exception":false,"start_time":"2023-12-30T23:51:08.45516","status":"completed"},"tags":[]},"source":["#title: \"Analyzing Amazon Food Reviews: Unveiling Consumer Insights\"\n","#author: \"Gizem GÃ¼leli\"\n","#date: \"2023-12-27\"\n","\n","\n","\n","## Overview\n","\n","####Delving into the intricate web of Amazon food reviews, we embarked on a journey to unravel customer sentiments, identify popular product categories, and uncover word associations that define the narrative. Our dataset, a co-purchasing network extracted from Amazon, comprised a staggering 262,111 nodes and 1,234,877 edges, offering a rich tapestry of interconnected insights.\n","##The dataset, representing a co-purchasing network, was thoroughly preprocessed to ensure the integrity of the text data. The analysis involved examining word frequencies, bigrams, and skip-grams to unravel patterns and associations within the reviews.\n","\n","##install and read neccessary libraries\n","\n","library(dplyr)\n","library(glue)\n","library(cowplot)\n","library(magrittr)\n","library(plotly)\n","library(tidyverse)\n","library(widyr)\n","\n","# Text Mining\n","library(tm)\n","library(wordcloud)\n","# Network Analysis\n","library(igraph)\n","# Network Visualization (D3.js)\n","library(networkD3)\n","\n","# JSON manipulation\n","library(rjson)\n","\n","# Text analysis\n","library(tidytext)\n","\n","\n","# convert our text into a corpus\n","library(tm)\n","library(SnowballC)\n","\n","## Data\n","\n","##Amazon Food Reviews Dataset represents an Amazon product co-purchasing network collected by crawling the Amazon website. It is based on the \"Customers Who Bought This Item Also Bought\" feature, forming directed edges between products frequently co-purchased. The data was collected in March 02, 2003.\n","\n","\n","data <- read.csv(\"Reviews.csv\", stringsAsFactors = FALSE)\n","\n","head(data)\n","\n","\n","\n","### Data Cleaning and Preprocessing\n","\n","#We performed data preprocessing and cleaning for a dataset. Initially,we focused on selecting relevant columns such as Id, ProductId, UserId, HelpfulnessNumerator, HelpfulnessDenominator, Score, Time, Summary, and Text while excluding any rows with missing values. This step ensured that we retained essential information for our analysis while maintaining data integrity. Subsequently, we prepared the text data within the reviews.df dataframe. Key transformations were applied to the Text column, including converting text to lowercase, removing unwanted characters such as newline characters (\\\\n), and eliminating specific patterns like HTML entities (&amp). Additionally, we addressed URLs by removing them along with hashtags and account mentions from the text.\n","##The aim of these preprocessing steps was to create a clean and standardized text corpus suitable for subsequent text mining and analysis tasks. we ensured a more consistent and focused textual dataset. Later we created a text corpus based on the Text column of the reviews.df dataframe. The corpus was processed through a sequence of transformations such as converting all text to lowercase, removing Punctuation, removing numbers, removing English stop words Words, and eliminating the Extra whitespaces.\n","###These preprocessing steps are essential for improving the quality of text data, making it conducive to tasks such as sentiment analysis, topic modeling, or network analysis. The resulting reviews.df dataframe serves as a refined foundation for gaining insights into the underlying textual content of the dataset. \n","#Subsequently, the cleaned corpus was applied to the reviews.df dataframe, replacing the original Text column with the processed version (out)\n","\n","\n","#  Select non-user related data and Clean the missing data\n","cleaned_data <- data %>%\n","  select(Id, ProductId,UserId, HelpfulnessNumerator,HelpfulnessDenominator, Score, Time, Summary, Text) %>%\n","  drop_na()  # Remove any rows with missing values\n","\n","# text preparation\n","reviews.df <- cleaned_data %>% \n","  # Convert to lowercase. \n","  mutate(Text = Text %>% str_to_lower) %>% \n","  # Remove unwanted characters. \n","  mutate(Text= Text %>% str_remove_all(pattern = '\\\\n')) %>% \n","  mutate(Text = Text %>% str_remove_all(pattern = '&amp')) %>% \n","  mutate(Text = Text %>% str_remove_all(pattern = 'https://t.co/[a-z,A-Z,0-9]*')) %>% \n","  mutate(Text = Text %>% str_remove_all(pattern = 'http://t.co/[a-z,A-Z,0-9]*')) %>% \n","  mutate(Text = Text %>% str_remove_all(pattern = 'https')) %>% \n","  mutate(Text = Text %>% str_remove_all(pattern = 'http')) %>% \n","  # Remove hashtags.\n","  mutate(Text = Text %>% str_remove_all(pattern = '#[a-z,A-Z]*')) %>% \n","  # Remove accounts.\n","  mutate(Text = Text %>% str_remove_all(pattern = '@[a-z,A-Z]*'))\n","\n","\n","#Creating corpus\n","corpus <-  VCorpus(x = VectorSource(x = reviews.df$Text))\n","\n","# Perform additional text cleaning on corpus\n","clean_corpus <- corpus %>%\n","  tm_map(content_transformer(tolower)) %>%\n","  tm_map(removePunctuation) %>%\n","  tm_map(removeNumbers) %>%\n","  tm_map(removeWords, stopwords(\"english\")) %>%\n","  tm_map(stripWhitespace)\n","\n","# Text from corpus to the vector out\n","\n","out <- sapply(clean_corpus, function(x){x$content})\n","# out\n","\n","is.vector(out)\n","\n","\n","#replacing the original Text column with the processed version\n","\n","reviews.df %<>% mutate(Text = out)\n","\n","\n","\n","## Analyzing Word Frequency: What Words Echo Loudest?\n","\n","#In this section, we delve into the textual content of the reviews to discern the most frequently occurring words. The initial step involves creating a word frequency table (word.count) that showcases the top words and their respective counts. However, during execution, a warning related to outer names for unnamed scalar atomic inputs might appear. Despite this warning, the results remain accurate.\n","##The presented output reveals the top 10 words and their frequencies in the Amazon product reviews dataset. Notably, the term \"br\" with 16305 occurrences raises a flag for further investigation, as  it might be an artifact or noise introduced during the text cleaning and tokenization process. Common positive descriptors such as \"good,\" \"great,\" and specific product-related terms like \"coffee\" dominate the list, emphasizing the positive sentiment and product-focused nature of the reviews. Understanding these prevalent words guides subsequent threshold-setting processes, ensuring the exclusion of noise and irrelevant terms for more meaningful analyses.\n","\n","# Counting the most popular words in the reviews\n","stopwords.df <- tibble(\n","  word = c(stopwords(kind = 'en'))\n","  )\n","words.df <- reviews.df %>% \n","  unnest_tokens(input = Text, output = word) %>% \n","  anti_join(y = stopwords.df, by = 'word')\n","\n","word.count <- words.df %>% count(word, sort = TRUE)\n","\n","word.count %>% head(10)\n","\n","### Visualization  of the most frequently occurring words \n","\n","#In this section, our objective was to visually represent the most frequently occurring words in the reviews. We initiated the process by calculating the frequency of each word after tokenizing and eliminating stopwords from the review text, resulting in the creation of the word.count dataframe containing words and their respective frequencies. To highlight the most significant words, a count threshold of 3000 was established, guiding the subsequent analysis. The top word counts were then visualized through a bar plot (plt), where the x-axis denoted words and the y-axis represented their frequencies, showcasing only those surpassing the defined threshold. To further enhance the interactive exploration of the data, the bar plot was transformed into an interactive plot using ggplotly(). Additionally, a word cloud was generated using the wordcloud package, presenting a visually compelling representation of the most frequent words. The color palette chosen for the word cloud aimed to improve visibility and overall aesthetics. Throughout these steps, the determination of thresholds was intricately linked to the analysis of the output data at each stage, ensuring a judicious balance between inclusivity and the emphasis on pertinent information in the reviews.\n","\n","\n","# visualize these counts in a bar plot\n","plt <- word.count %>% \n","  # Set count threshold. \n","  filter(n > 3000) %>%\n","  mutate(word = reorder(word, n)) %>%\n","  ggplot(aes(x = word, y = n)) +\n","  theme_light() + \n","  geom_col(fill = 'lightcoral', alpha = 0.8) +\n","  xlab(NULL) +\n","  coord_flip() +\n"," ggtitle(\"Top Word Count\") +\n","  theme(plot.title = element_text(size = 20, color = 'red', face = 'bold'))\n","\n","\n","plt %>% ggplotly()\n","\n","wordcloud(\n","  words = word.count$word, \n","  freq = word.count$n, \n","  min.freq = 3800, \n","  colors = brewer.pal(8, 'Dark2')\n",")\n","\n","\n","## Network Analyze\n","\n","\n","### Navigating Bigrams: Unveiling Word Companionships\n","\n","#In the bi-gram section, we analyze pairwise occurrences of words appearing together in the text. The goal is to identify meaningful associations between words and create a network representation of bigram relationships. We filter out stop words and white spaces, count the occurrences of each bigram, and visualize the distribution of their weights. To manage the skewed distribution, a log transformation is applied. A threshold is set to define the minimal weight allowed in the graph, and the resulting bigram network is visualized. The network is represented using a force-directed layout, with custom colors for vertex labels and a size scale based on the degree of each node.\n","\n","##The output displays the top bigrams based on their weights, representing the frequency of occurrence. Each row consists of two words (word1 and word2) forming a bigram, and the corresponding weight indicates the number of times that bigram appears in the text. For example, the bigram \"highly recommend\" has a weight of 898, indicating that this phrase is frequently used in the reviews. Similarly, other bigrams like \"peanut butter,\" \"taste like,\" and \"gluten-free\" are also common expressions. The table provides insights into meaningful word associations and recurring phrases in the analyzed text.\n","\n","\n","\n","# count pairwise occurrences of words which appear together in the text\n","bi.gram.words <- reviews.df %>% \n","  unnest_tokens(\n","    input = Text, \n","    output = bigram, \n","    token = 'ngrams', \n","    n = 2\n","  ) %>% \n","  filter(! is.na(bigram))\n","\n","bi.gram.words %>% \n","  select(bigram) %>% \n","  head(10)\n","\n","\n","\n","# filter for stop words and remove white spaces\n","bi.gram.words %<>% \n","  separate(col = bigram, into = c('word1', 'word2'), sep = ' ') %>% \n","  filter(! word1 %in% stopwords.df$word) %>% \n","  filter(! word2 %in% stopwords.df$word) %>% \n","  filter(! is.na(word1)) %>% \n","  filter(! is.na(word2)) \n","\n","# group and count by bigram\n","bi.gram.count <- bi.gram.words %>% \n","  count(word1, word2, sort = TRUE) %>% \n","  # We rename the weight column so that the \n","  # associated network gets the weights (see below).\n","  rename(weight = n)\n","\n","bi.gram.count %>% head()\n","\n","\n","### Visualization of Bi-gram Results\n","\n","#In the normal bigram weight distribution plot, in the beginning we observe that the y-axis displays counts in scientific notation (e.g., 2e+05), which can be visually challenging. To address this, we modify the y-axis to display counts in a more readable format. Additionally, we enhance the visual appeal of the plots by introducing vibrant and customized colors, deviating from the default settings to make the graphs more engaging.\n","##Having plotted the normal bigram weight distribution, we observed a right-skewed distribution that prompted us to address readability issues in the y-axis counts by modifying the format. Due to the skewed nature of the distribution, we opted for a logarithmic transformation in the log-transformed bigram weight distribution plot. This transformation normalize the data  for better visualization and allowed better interpretation of the weight patterns in the bigrams.\n","###From these results, we confronted a trade-off between network inclusivity and the strength of connections. A higher threshold (around 500-550) emphasized fewer bigrams with robust associations, fostering a focused network. Conversely, a lower threshold (around 100-150) prioritized a more extensive network but potentially weaker connections. Opting for a medium threshold (300-350) struck a balance, incorporating a moderate number of bigrams (41-58) with average weights around 0.27. This choice aimed to capture a meaningful yet comprehensive representation of word associations in the Amazon product reviews, offering a nuanced perspective that considers both network strength and inclusivity. The chosen threshold, set at 350, was applied to the weights. We normalized the weights for better visualization by scaling them with a global factor. Using igraph, we created a network object and visualized it, adjusting parameters such as vertex size, color, and edge width. The resulting force-directed network plot provides a comprehensive view of significant bigram relationships, contributing to a deeper understanding of patterns within the dataset.\n","\n","\n","# Plotting the distribution of bigram weights (Normal)\n","bi.gram.count %>% \n","  ggplot(mapping = aes(x = weight)) +\n","    theme_minimal() +\n","    geom_histogram(color = \"skyblue\", fill = \"lightblue\", bins = 30) +\n","    scale_y_continuous(labels = scales::comma) +  # Display counts with commas\n","    labs(\n","      title = \"Distribution of Bigram Weights\",\n","      x = \"Weight\",\n","      y = \"Count\"\n","    )\n","\n","\n","\n","# very skewed, for visualization purposes it might be a good idea to perform a transformation\n","# Plotting the distribution of log-transformed bigram weights\n","bi.gram.count %>% \n","  mutate(log_weight = log(weight + 1)) %>% \n","  ggplot(mapping = aes(x = log_weight)) +\n","    theme_minimal() +\n","    geom_histogram(color = \"salmon\", fill = \"lightcoral\", bins = 30) +\n","    scale_y_continuous(labels = scales::comma) +  # Display counts with commas\n","    labs(\n","      title = \"Log-Transformed Distribution of Bigram Weights\",\n","      x = \"Log-Transformed Weight\",\n","      y = \"Count\"\n","    )\n","\n","\n","# Set the threshold\n","\n","threshold <- 350\n","\n","# For visualization purposes we scale by a global factor. \n","ScaleWeight <- function(x, lambda) {\n","  x / lambda\n","}\n","\n","network <-  bi.gram.count %>%\n","  filter(weight > threshold) %>%\n","  mutate(weight = ScaleWeight(x = weight, lambda = 2E3)) %>% \n","  graph_from_data_frame(directed = FALSE)\n","\n","network\n","\n","is.weighted(network)\n","\n","# Generate a palette of colors for vertex labels\n","label_colors <- rainbow(length(V(network)))\n","\n","# Plot the network with custom colors for vertex labels\n","plot(\n","  network, \n","  vertex.size = 1,\n","  vertex.label.color = label_colors,\n","  vertex.label.cex = 0.7, \n","  vertex.label.dist = 1,\n","  vertex.color = 'white',  # Set vertex color to white\n","  edge.color = 'gray', \n","  main = 'Bigram Count Network', \n","  sub = glue('Weight Threshold: {threshold}'), \n","  alpha = 50\n",")\n","\n","\n","#Set threshold\n","threshold <- 350\n","\n","network <-  bi.gram.count %>%\n","  filter(weight > threshold) %>%\n","  graph_from_data_frame(directed = FALSE)\n","\n","# Store the degree.\n","V(network)$degree <- strength(graph = network)\n","# Compute the weight shares.\n","E(network)$width <- E(network)$weight/max(E(network)$weight)\n","\n","# Create networkD3 object.\n","network.D3 <- igraph_to_networkD3(g = network)\n","# Define node size.\n","network.D3$nodes %<>% mutate(Degree = (1E-2)*V(network)$degree)\n","# Degine color group (I will explore this feature later).\n","network.D3$nodes %<>% mutate(Group = 1)\n","# Define edges width. \n","network.D3$links$Width <- 10*E(network)$width\n","\n","forceNetwork(\n","  Links = network.D3$links, \n","  Nodes = network.D3$nodes, \n","  Source = 'source', \n","  Target = 'target',\n","  NodeID = 'name',\n","  Group = 'Group', \n","  opacity = 0.9,\n","  Value = 'Width',\n","  Nodesize = 'Degree', \n","  # We input a JavaScript function.\n","  linkWidth = JS(\"function(d) { return Math.sqrt(d.value); }\"), \n","  fontSize = 12,\n","  zoom = TRUE, \n","  opacityNoHover = 1\n",")\n","\n","```\n","\n","\n","### Skip-grams: Bridging Gaps in Word Connections\n","\n","##In this section, we extend our analysis by considering skipgrams, which allow for a \"jump\" in the word count. We extract skipgrams containing two words, count their occurrences, and visualize the resulting network. Similar to the bigram section, we apply a threshold to filter the network and focus on meaningful relationships. The dynamic plot is generated to highlight the most significant connections within the skipgram network, allowing for a more comprehensive exploration of word associations in the text data. The thresholds and parameters are chosen based on the outputs and the specific goals of the analysis.\n","\n","##Analyzing the results, a medium threshold of 300 appears to strike a reasonable balance between the number of skip-grams retained and the average weight. With this threshold, the network includes 80 skip-grams with an average weight of 480.88. This choice aims to provide a meaningful representation of associations in the skip-grams, capturing a moderate level of connection strength while still being inclusive enough to offer insights into the data\n","\n","\n","# consider skipgrams, which allow a âjumpâ in the word count\n","\n","skip.window <- 2\n","\n","skip.gram.words <- reviews.df %>% \n","  unnest_tokens(\n","    input = Text, \n","    output = skipgram, \n","    token = 'skip_ngrams', \n","    n = skip.window\n","  ) %>% \n","  filter(! is.na(skipgram))\n","\n","# consider the example review\n","\n","reviews.df %>% \n","  slice(4) %>% \n","  pull(Text)\n","\n","# The skipgrams are\n","skip.gram.words %>% \n","  select(skipgram) %>% \n","  slice(10:20)\n","\n","\n","\n","# count the skipgrams containing two words\n","library(ngram)\n","\n","skip.gram.words$num_words <- skip.gram.words$skipgram %>% \n","  map_int(.f = ~ ngram::wordcount(.x))\n","\n","skip.gram.words %<>% filter(num_words == 2) %>% select(- num_words)\n","\n","skip.gram.words %<>% \n","  separate(col = skipgram, into = c('word1', 'word2'), sep = ' ') %>% \n","  filter(! word1 %in% stopwords.df$word) %>% \n","  filter(! word2 %in% stopwords.df$word) %>% \n","  filter(! is.na(word1)) %>% \n","  filter(! is.na(word2)) \n","\n","skip.gram.count <- skip.gram.words  %>% \n","  count(word1, word2, sort = TRUE) %>% \n","  rename(weight = n)\n","\n","skip.gram.count %>% head()\n","\n","\n","# Create an empty data frame to store results\n","threshold_summary <- data.frame()\n","\n","# Loop through different threshold values\n","for (threshold in c(100, 150, 200, 250, 300, 350, 400, 450, 500, 550)) {\n","  \n","  # Filter skip-gram count based on the threshold\n","  filtered_skip_grams <- skip.gram.count %>%\n","    filter(weight > threshold)\n","  \n","  # Calculate the number of bigrams and average weight\n","  num_bigrams <- nrow(filtered_skip_grams)\n","  avg_weight <- mean(filtered_skip_grams$weight)\n","  \n","  # Add results to the summary data frame\n","  threshold_summary <- rbind(threshold_summary, c(threshold, num_bigrams, avg_weight))\n","}\n","\n","# Rename columns\n","colnames(threshold_summary) <- c(\"Threshold\", \"Number_of_Bigrams\", \"Average_Weight\")\n","\n","# Display the summary table\n","print(threshold_summary)\n","\n","# Visualize the results\n","library(ggplot2)\n","\n","ggplot(threshold_summary, aes(x = Threshold, y = Number_of_Bigrams, color = Average_Weight)) +\n","  geom_point(size = 3) +\n","  scale_color_gradient(low = \"blue\", high = \"red\") +\n","  labs(title = \"Threshold Impact on Skip-grams\", x = \"Threshold\", y = \"Number of Bigrams\")\n","\n","\n","\n","# dynamic plot (RStudio)\n","# Treshold\n","threshold <- 300\n","\n","network <-  skip.gram.count %>%\n","  filter(weight > threshold) %>%\n","  graph_from_data_frame(directed = FALSE)\n","\n","# Select biggest connected component.  \n","V(network)$cluster <- clusters(graph = network)$membership\n","\n","cc.network <- induced_subgraph(\n","  graph = network,\n","  vids = which(V(network)$cluster == which.max(clusters(graph = network)$csize))\n",")\n","\n","# Store the degree.\n","V(cc.network)$degree <- strength(graph = cc.network)\n","# Compute the weight shares.\n","E(cc.network)$width <- E(cc.network)$weight/max(E(cc.network)$weight)\n","\n","# Create networkD3 object.\n","network.D3 <- igraph_to_networkD3(g = cc.network)\n","# Define node size.\n","network.D3$nodes %<>% mutate(Degree = (1E-2)*V(cc.network)$degree)\n","# Degine color group (I will explore this feature later).\n","network.D3$nodes %<>% mutate(Group = 1)\n","# Define edges width. \n","network.D3$links$Width <- 10*E(cc.network)$width\n","\n","forceNetwork(\n","  Links = network.D3$links, \n","  Nodes = network.D3$nodes, \n","  Source = 'source', \n","  Target = 'target',\n","  NodeID = 'name',\n","  Group = 'Group', \n","  opacity = 0.9,\n","  Value = 'Width',\n","  Nodesize = 'Degree', \n","  # We input a JavaScript function.\n","  linkWidth = JS(\"function(d) { return Math.sqrt(d.value); }\"), \n","  fontSize = 12,\n","  zoom = TRUE, \n","  opacityNoHover = 1\n",")\n","\n","\n","\n","### Compare the ngram results\n","\n","#Comparing the results of the top 10 bigrams and skip-grams, we observe some similarities and differences. In both cases, there are expressions related to taste (\"taste like,\" \"peanut butter\"), recommendations (\"highly recommend,\" \"really like\"), and product experiences (\"ive tried,\" \"dog food\"). However, there are unique combinations as well, such as \"grocery store\" in bigrams and \"tastes like\" in skip-grams.\n","##The weights indicate the strength of association between the words. For instance, \"taste like\" has a higher weight in skip-grams (1156) compared to \"highly recommend\" (898) in bigrams, suggesting a potentially stronger connection between these words in the context of reviews.\n","###These comparisons help us understand how different n-gram approaches capture distinct patterns and associations in the dataset, providing valuable insights into the language used by reviewers. Adjusting thresholds and exploring various n-gram combinations allow for a more nuanced analysis of word relationships in the reviews.\n","\n","\n","\n","bi.gram.count %>% head(10)\n","skip.gram.count %>% head(10)\n","\n","\n","## Insights and Takeaways\n","\n","### Taste Dictates Trends\n","\n","#Expressions related to taste (\"taste like,\" \"really good\") echoed prominently across bigrams and skip-grams. The emphasis on sensory experiences underlines the pivotal role taste plays in shaping consumer perceptions.\n","\n","### Recommendations Echo Loudly\n","\n","#\"Highly recommend\" resonated strongly, underscoring the influential power of recommendations in shaping consumer choices. The echo of positive endorsements reverberated throughout the dataset.\n","\n","### Product-Specific Conversations\n","\n","#Distinctive product references like \"dog food\" and \"grocery store\" surfaced, indicating specific product categories driving conversations. Understanding these nuances is crucial for businesses aiming to align with consumer preferences.\n","\n","\n","## In Closing\n","\n","#Our exploration into the Amazon food reviews goes beyond the surface of individual words, revealing a intricate mosaic of consumer sentiments and preferences. Whether it's the flavor-centric vocabulary or the reverberation of recommendations, each word contributes to a narrative woven intricately in the vast tapestry of Amazon's food reviews. As businesses traverse this linguistic landscape, these insights act as a guiding compass, providing a profound understanding of their customers. In the orchestration of reviews, every word assumes a pivotal role, and our analysis aims to decode the harmonious melody embedded within.\n","\n","## REFERENCES\n","\n","#J. Leskovec, L. Adamic, and B. Adamic. The Dynamics of Viral Marketing. ACM Transactions on the Web (ACM TWEB), 1(1), 2007.\n","\n","\n"]},{"cell_type":"markdown","id":"fd585099","metadata":{"papermill":{"duration":0.001758,"end_time":"2023-12-30T23:51:08.461585","exception":false,"start_time":"2023-12-30T23:51:08.459827","status":"completed"},"tags":[]},"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":18,"sourceId":2157,"sourceType":"datasetVersion"}],"dockerImageVersionId":30618,"isGpuEnabled":false,"isInternetEnabled":true,"language":"r","sourceType":"notebook"},"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"codemirror_mode":"r","file_extension":".r","mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"4.0.5"},"papermill":{"default_parameters":{},"duration":3.056334,"end_time":"2023-12-30T23:51:08.584521","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-12-30T23:51:05.528187","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}